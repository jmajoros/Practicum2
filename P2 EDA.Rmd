---
title: "P2 - TechR"
author: "Skip Potts"
date: "3/8/2021"
output: html_document
---
# Looking more at missing data from "contact"
```{r}
library(VIM)
library(mice)
setwd("C:/Users/skip/OneDrive/Desktop/Spring 21/S690/")
P2<-read.csv("C:/Users/skip/OneDrive/Desktop/Spring 21/S690/P2.csv")

#change unknown and blanks to NA's
dat2 <- read.csv("C:/Users/skip/OneDrive/Desktop/Spring 21/S690/P2.csv", header=T, na.strings=c(c("unknown", ""),"NA"))
#dropping non-obs responses
drop<-c("day", "month", "poutcome", "previous", "duration", "campaign", "pdays", "age", "balance", "price") #tracking our work, not necessarily relevant to our purposes.  I also removed variables which were complete.
dat2<-dat2[,!(names(dat2) %in% drop)]

Practicum_2_aggr = aggr(dat2, col=mdc(1:2), numbers=TRUE, sortVars=TRUE, labels=names(dat2), cex.axis=.7, gap=1, ylab=c("Proportion of missingness","Missingness Pattern")) #check proportion of missingness and missing patterns, contact has the most missing values

md.pattern(dat2, rotate.names = TRUE) #check the pattern of missing values, visual useless, console is useful

library(dplyr)
dat3<-filter(dat2, is.na(dat2$contact))
nrow(dat3)
drop<-c("contact") #removing contact
dat4<-dat3[,!(names(dat3) %in% drop)]
dat4<-na.omit(dat4)
dat4$y<-factor(dat4$y)
summary(dat4)
#still can't figure out why y is so strange on contact...
```

Contact behaves strangely missing nearly 4 times as much data as the next variable, but being the only one for which the level of unknown was truly significantly different.  Based on knowing that unknown contact is very different from other values, we might infer that people that say "no" are more likely to also not tell us their mode of contact - probably not the reverse.  We might also guess that contact is among the last pieces of info asked and people may have tired of answering at this point.  Essentially, people that are more willing to answer all or most of the questions, are more likely to say yes...but this doesn't really seem to add up either.  We may just need to say it is not understood, but there is a connection and it could be explored further in the future if the client is interested.

# Univriate EDA and Bivariate EDA 
```{r}
library(car)
hist(P2$age) # not great
hist(P2$balance) # not so good, seems like we have negatives coded on accident here.
P2$balance2<-abs(P2$balance)+1  #changing to absolute value
#maybe transform there using box-cox
bc = powerTransform(cbind(age, balance2) ~ 1, P2) #GS would not work here, maybe change to a factor...
summary(bc)
summary(P2)
plot(P2$age,P2$balance )
```


#Specific Aim 1:  How are the two datasets (from Table 1 and Table 2)  different and why?

## 1.1 
Product Response will be considered as the response variable where “yes” means an individual indicated that they would purchase the USB Toaster and “no” indicated they would not.  Common variables between the two datasets are: age, marital status, profession, education level, whether an individual has a mortgage or not, whether an individual has a personal loan other than a mortgage, the balance of any non-mortgage personal loan, and primary phone (cell or telephone).  Additionally, whether an individual is 60 days delinquent or in credit default may also be comparable between datasets. 

## 1.2  
Common variables between the two datasets will be compared to determine whether they represent the same population.  We will use proportion tests that compare the percentages of certain types of demographic data and the total number of respondents for the dataset to determine if the datasets represent the same population or populations that are fundamentally different.
```{r}
#changing all to factor and merging blanks and unknowns into a single factor.
P2$job<-factor(sub("^$", "unknown", P2$job))
P2$marital<-factor(sub("^$", "unknown", P2$marital))
P2$education<-factor(sub("^$", "unknown", P2$education))
P2$default<-factor(sub("^$", "unknown", P2$default))
P2$housing<-factor(sub("^$", "unknown", P2$housing))
P2$loan<-factor(sub("^$", "unknown", P2$loan))
P2$contact<-factor(sub("^$", "unknown", P2$contact))
P2$y<-factor(P2$y)
summary(P2)

#prop.test to see if marital status is sig diff. in two pops.
prop.test(x=c(421*0.14,26543), n=c(421, 45211),
          conf.level=0.95)
#Very different

#prop.test to see if blue-collar is sig diff. in two pops.
prop.test(x=c(421*0.02,9064), n=c(421, 45211),
          conf.level=0.95)
#Very different

#prop.test to see if student is sig diff. in two pops.
table(P2$job)
prop.test(x=c(421*0.65,876), n=c(421, 45211),
          conf.level=0.95)
#Very different

#prop.test to see if unemployed/unknown is sig diff. in two pops.
table(P2$job)
prop.test(x=c(421*0.047,3309+1222), n=c(421, 45211),
          conf.level=0.95)
#Very different

#prop.test to see if education is sig diff. in two pops.
table(P2$education)
prop.test(x=c(421*0.83,13015), n=c(421, 45211),
          conf.level=0.95)
#Very different

#prop.test to see if mortgage is sig diff. in two pops.
prop.test(x=c(421*0.056,24656), n=c(421, 45211),
          conf.level=0.95)
#Very different

#prop.test to see if 60+/default is sig diff. in two pops.
prop.test(x=c(421*0.44,784), n=c(421, 45211),
          conf.level=0.95)
#Very different

#prop.test to see if contact is sig diff. in two pops.
prop.test(x=c(421*0.36,29262), n=c(421, 45211),
          conf.level=0.95)
#Very different
```
Here we can talk about how these are different and why.  Their population somehow is in debt, college, men, who use cells phones less, and are fiscally irresponsible...even if they want to buy this product, they may not have the money to do so.  

This gets to measure accuracy which we will talk about below in the psychometrics and other research (below)

## 1.3  
Based on the results of the above analyses, we will determine how and some potential reasons why the dataset are different (if we find they are) using both statistical reasoning and potentially some psychometric theory and research.

We can talk about my above notes here.  Sampling error skewing the population.  Also sources of error on psychological tests (intent to buy) time differences, personal variation, task not reflective of event.  More on this last one, research indicates surveys and focus groups aren't great barometers for purchasing.  A potential better metric will be discussed later, but we will move forward with the data we have.


### (Rough Draft of some text below. This might be better as part of the conclusion and suggesting a different approach to future research in this area.)

It has been noted that surveys asking if people will hypothetically buy a product with money they may not have, is not necessarily a good measure for intent to buy.  In researching the viability of this measure, our consulting team found a particular source of interest that specifically investigated durable homewares as part of its data.  According to research published in the Journal of the American Statistical Association by F. Thomas Juster, "consumer intentions to buy are inefficient predictors of purchase rates because they do not provide accurate estimates of mean purchase probability." (hyperlink: https://www.tandfonline.com/doi/abs/10.1080/01621459.1966.10480897) 

The reasons for this may be rooted in the fact that measuring an intent to purchase is a psychological construct and these measures have commonly known sources of unreliability*, all of which apply in this context.  The three common sources of performance variation are:
1) The person tested may change from one testing to the next
2) The task measured and the behavior may be different
3) The limited sample of the behavior results in an unstable and undependable score.

While it is not the intent to delve deeply into psychometrics, a brief aside exploring the potential reliability of the product intent variable in the context of the research above might be fruitful for understanding the task at hand of examining the probability of product success.

  In the article by Juster, the predominant source of error cited that made customer buying intentions problematic.  As Juster puts it, "intentions surveys cannot detect movements in mean probability among nonin- tenders, who account for the bulk of actual purchases and for most of the time-series variance in purchase rates." Meaning that though our data gives a "yes" or "no" response, the reality of the situation is different.  This gets at the heart of each of the above sources of unreliability.
  First, there is personal variation over time, whether through change in the person or the context of the persons life.  Second, there is a difference in task - what we really want is the probability of a purchase, but the data forces a binary situation on the individuals.  Finally, for each individual we have only one measure, which is not the most reliable measure.
  Juster's article finds that having individuals rate the probability they will purchase a product is a better measure of predictability.    While this does not completely fix any of the problems noted above, it allows for a better representation of the uncertainty in an individuals choice and thus makes for a better measure.

Footnote
$_{\textrm{*Thorndike and Thorndike-Christ, (2010).  Measurement and Evaluation in Psychology and Education (8th Ed.). Pearson}}$


## 1.4  
After viewing the results of the analyses above, we will determine how to proceed with each dataset in the subsequent specific aims.

We will proceed using our data set as the primary means of analysis, while also incorporating in the client data where suitable.

#Specific Aim 2:  What is the likelihood, based on all available data, that the USB Toaster will be a success?

## 2.1  
Using the data decided upon in specific aim 1 (SA1), we will focus on the average number of respondents that indicated they would purchase the USB Toaster.

```{r}
table(P2$y)
5289/39922 #positive product response
```

## 2.2  
Confidence intervals will be calculated for this average to give the potential range of sales within a 95% probability band.  We will generate confidence intervals based on both frequency of response in the final dataset and potentially a Bayesian estimate of the average using the data provided by the client as a baseline and adding to that the new information gathered in Table 2.

```{r}
#frequentist Table 1 and 2
data1<-c(replicate(212+5289, 1), replicate(209+39922, 0))
x.std.error <- sd(data1)/sqrt(45211+421)
mean(data1) + qt(0.025, 45211+420) * x.std.error
mean(data1) + qt(0.975, 45211+420) * x.std.error

#frequentist only Table 2
P2$y.n<-as.numeric(P2$y)-1
x.std.error <- sd(P2$y.n)/sqrt(45211)
lm(y.n~1, P2)
mean(P2$y.n) + qt(0.025, 45210) * x.std.error
mean(P2$y.n) + qt(0.975, 45210) * x.std.error

#BAYESIAN


#calcLikelihoodForProportion <- function(successes, total)
  {
     curve(dbinom(successes,total,x)) # plot the likelihood
}
#calcLikelihoodForProportion(5289, 45211)

#credible interval
qbeta(c(0.025, 0.975),212+5289, 209+39922)

#plotting function for prior, likelihood and posterior
calcPosteriorForProportion <- function(successes, total, a, b)
  {
     # Adapted from triplot() in the LearnBayes package
     # Plot the prior, likelihood and posterior:
     likelihood_a = successes + 1; likelihood_b = total - successes + 1
     posterior_a = a + successes;  posterior_b = b + total - successes
     theta = seq(0.005, 0.995, length = 500)
     prior = dbeta(theta, a, b)
     likelihood = dbeta(theta, likelihood_a, likelihood_b)
     posterior  = dbeta(theta, posterior_a, posterior_b)
     m = max(c(prior, likelihood, posterior))
     plot(theta, posterior, type = "l", ylab = "Density",, xlab="Estimated Positive Product Response",
          lty = 2, lwd = 3, main = "", ylim = c(0, m), col = "red")
     lines(theta, likelihood, lty = 1, lwd = 3, col = "blue")
     lines(theta, prior, lty = 3, lwd = 3, col = "green")
     legend(x=0.75,y=m, c("Prior", "Likelihood", "Posterior"), lty = c(3, 1, 2),
          lwd = c(3, 3, 3), col = c("green", "blue", "red"))
     # Print out summary statistics for the prior, likelihood and posterior:
     calcBetaMode <- function(aa, bb) { BetaMode <- (aa - 1)/(aa + bb - 2); return(BetaMode); }
     calcBetaMean <- function(aa, bb) { BetaMean <- (aa)/(aa + bb); return(BetaMean); }
     calcBetaSd   <- function(aa, bb) { BetaSd <- sqrt((aa * bb)/(((aa + bb)^2) * (aa + bb + 1))); return(BetaSd); }
     prior_mode      <- calcBetaMode(a, b)
     likelihood_mode <- calcBetaMode(likelihood_a, likelihood_b)
     posterior_mode  <- calcBetaMode(posterior_a, posterior_b)
     prior_mean      <- calcBetaMean(a, b)
     likelihood_mean <- calcBetaMean(likelihood_a, likelihood_b)
     posterior_mean  <- calcBetaMean(posterior_a, posterior_b)
     prior_sd        <- calcBetaSd(a, b)
     likelihood_sd   <- calcBetaSd(likelihood_a, likelihood_b)
     posterior_sd    <- calcBetaSd(posterior_a, posterior_b)
     print(paste("mode for prior=",prior_mode,", for likelihood=",likelihood_mode,", for posterior=",posterior_mode))
     print(paste("mean for prior=",prior_mean,", for likelihood=",likelihood_mean,", for posterior=",posterior_mean))
     print(paste("sd for prior=",prior_sd,", for likelihood=",likelihood_sd,", for posterior=",posterior_sd))
}

#plotting the prior, likelihood and posterior
calcPosteriorForProportion(5289, 45211, 212, 209)

#credible interval
qbeta(c(0.025, 0.975),212+5289, 209+39922) # 95% of the time, we would expect the true mean would fall in this interval
qbeta(c(0.005, 0.995),212+5289, 209+39922) # 99% of the time, we would expect the true mean would fall in this interval
qbeta(c(0.0005, 0.9995),212+5289, 209+39922) # 999 out of 1000 times, we would expect the true mean would fall in this interval
```
Discuss the potential means here and meaning of Bayesian estimation of the mean.
	
## 2.3  
Using the calculated confidence intervals, we will determine the probability that the USB Toaster will be profitable.

```{r}
1-pbeta(.2413, 212+5289, 209+39922)
```
Essentially a 0% chance with the target market.  Hope is not lost, better targeting and price raising might lead to a profit.

# Specific Aim 3: What are some useful demographic data points that might help inform a potential marketing campaign?

## 3.1  
Regardless of the findings of SA2 (probability of product profit or loss), we will construct a logistic regression model using product response as the outcome variable and a training set of data (part of the dataset).  A logistic regression model will describe the change in odds of a success (indication of intent to purchase) based on changes in the other variables.   

```{r}
#create training set
library(car)
library(faraway)
# 50% of the sample size
sample_size <- floor(0.5 * nrow(P2))
# set the seed to make the partition reproducible
set.seed(538)
train_ind <- sample(seq_len(nrow(P2)), size = sample_size)
train <- P2[train_ind, ] #training
test <- P2[-train_ind, ] #test

summary(train)
m0<-glm(y.n~1, family=binomial, train)
m1<-glm(y.n~.-y-price, family=binomial, train) #removed price because it is only off interest later.
#(also the model didn't converge with price)
sumary(m1)
# LRT for null vs full - null hypothesis is null is good 
pchisq(deviance(m0)-deviance(m1),df.residual(m0)-df.residual(m1),lower=FALSE) #full is better
# G^2 goodness of fit - null is model fits
pchisq(deviance(m1),df.residual(m1),lower=FALSE) #no evidence to reject null
drop1(m1, test="Chi") #full model is best by AIC and deviance


m2<-glm(y.n~I(age^.17)+I(balance2^(-.15))+job+marital+education+default+housing+loan+contact, family=binomial, train) #removed price because it is only off interest later.
sumary(m2)  #AIC indicates slightly better fit with tranformations.
drop1(m2, test="Chi") #age and default are not significant predictors in this model.

m3<-glm(y.n~I(balance2^(-.15))+job+marital+education+housing+loan+contact, family=binomial, train) 
summary(m3) #AIC is slightly improved
drop1(m3, test="Chi") # m3 seems to be the best model
```
Full model is best and it fits very well.

3.2  Diagnostic analysis will be performed to check the assumptions of the logistic model using residual plots, halfnorm plots and other relevant tests. Potential outliers or influential points will be examined with caution.

```{r}
library(alr4)
library(faraway)
halfnorm(residuals(m3)) #looks okay for a binomial, no big outliers
```


3.3 Tests will be conducted to see the accuracy of the model (correct classification rate) with respect to correctly identifying whether an individual will or will not purchase the product based on the other variables in the model using the test set (the data not used in the training set).

```{r}
train<-mutate(train, p.hat=predict(m1, type="response"), eta=predict(m3))
train <- mutate(train, predout=ifelse(p.hat < 0.5, "no", "yes"))
xtabs( ~ y + predout, train)
#correct spec rate for training 88.2%, but basically by saying no every time (1-0.117=0.883...)
(2+19935)/nrow(train)

thresh <- seq(0.01, 0.3, by = 0.01)
Class.rate <- numeric(length(thresh))
Sensitivity <- numeric(length(thresh))
Specificity <- numeric(length(thresh))
for(j in seq(along=thresh)){
  pp <- ifelse(train$p.hat < thresh[j],"no","yes")
  xx <- xtabs( ~ y + pp, train)
  Class.rate[j] <- (xx[1,1] + xx[2,2])/sum(xx)
  Sensitivity[j] <- xx[2,2]/(xx[2,1]+xx[2,2])
  Specificity[j] <- xx[1,1]/(xx[1,1]+xx[1,2])
}
matplot(thresh,cbind(Sensitivity,Specificity),type="l",xlab="Threshold",ylab="Proportion",lty=1:2)

plot(1-Specificity,Sensitivity,type="l")
abline(0,1,lty=2)
```
Not great at predicting...Specificity is good, sensitivity is terrible. So we know some things that make people more likely to buy, but don't have a great model.  Perhaps contingency tables will be better....we could still see how it performs on the test set or just use the full set above.  (Good verbage borrowed/altered from a text: Almost no cases are classified as "yes" because the probability of "no" almost always dominates the outcome. so the performance is not what we would like, but not surprising given the large proportion of "no" in each category.  Perhaps what is most notable is looking at the individuals predicted as "yes" for product response.)


## 3.4  
Contingency tables will also be constructed to determine if particular combinations of variables are especially lucrative possibilities or potentially poor demographics for sales.  While binomial regression (two end values, yes and no) will give us odds changes based on variable changes, contingency tables can give us strong insight into the relationship between predictive variables that could help in mirco-targeting potential buyers (more below)..

```{r}
ct.stuff<-function(fact1, fact2, data){
(ct = xtabs(P2$y.n ~ fact1 + fact2, data)) #yes count on education by employment
(ct2 = xtabs( ~ fact1 + fact2, data))
ct/ct2
}

ct.stuff(P2$education, P2$job, P2) #shows percentage yes for cross section.  We see students say yes much more frequently overall.  Over 24.13%...only place we see this - basically college or grad students
#retired also approaches critical, and is over for tertiary
#unemployed tertiary also decent at 19%

ct.stuff(P2$education, P2$marital, P2)  #single-tertiary at 18.3%, 

ct.stuff(P2$education, P2$default, P2)  #tertiary better, but not great.

ct.stuff(P2$education, P2$housing, P2)  #tertiary and no house @ 20%

ct.stuff(P2$education, P2$loan, P2)  #tertiary again, but not with a loan

ct.stuff(P2$education, P2$contact, P2)  #tertiary again performing well.

ct.stuff(P2$job, P2$marital, P2) #retired but not single, students not divorced- especially single, single and unemployed

ct.stuff(P2$job, P2$default, P2) #students unless in default, similar to retired (not as good, but okay)

ct.stuff(P2$job, P2$housing, P2) #retired non-home-owner, student non-homeowner, and unemployed non-homeowner

ct.stuff(P2$job, P2$loan, P2) #students without loans, retired without loans

ct.stuff(P2$job, P2$contact, P2)  #here's the caveat,  This weird group of unknown contacts
#other than this, students and retirees still come out okay.

ct.stuff(P2$marital, P2$default, P2)  #single is better...but nothing special

ct.stuff(P2$marital, P2$housing, P2)  #single non-homeowner, generally nonhomeowners are better

ct.stuff(P2$marital, P2$loan, P2) #single with no loan, and strangely, unknown-unknown

ct.stuff(P2$marital, P2$contact, P2) #single cell and divorced tele

ct.stuff(P2$default, P2$housing, P2) # no housing, no default.  We are starting to get a clearer picture of a demo type here.
# generally single, well-educated, students or retirees with no encumberance (financial or relationship).  Basically the picture of table 1.
ct.stuff(P2$default, P2$loan, P2) #zippo here.

ct.stuff(P2$default, P2$contact, P2) #cellular no default.  Again, unencumbered.

ct.stuff(P2$housing, P2$loan, P2) # no housing-no loan, and unknown-unknown

ct.stuff(P2$housing, P2$contact, P2) #non-homeowners - this product isn't about people on the go, it's about single people living in apartments.

ct.stuff(P2$contact, P2$loan, P2) #cell and no loan is best.

str(P2)
```


3.5  The contingency tables generated will be analyzed further to determine if specific variables are independent of others or have a relationship.  These results may allow the consultants to modify certain variables to be more descriptive and explanatory within the dataset.  This will potentially illuminate some variable relationships further (such as marital status and education’s combined effect on intent to purchase).

Done above.

# 3.6  
The final models of both methods described above will be interpreted to determine any useful demographic data that may be used to maximize profit by targeted marketing.  In the case of a probable loss in section SA2, this may allow for stronger profit potential based on focusing on the demographics identified in the models that are positively associated with intent to purchase.

```{r}
sumary(m3)  #we see student/retired, tertiary, divorced/single, no house, no loan
#we can interpret these, but I don't think it is necessary
#these are all in line with the cts
# we can use the general direction and size from logistic regression, but with specific % from cts to discuss
```


# Specific Aim 4:  What does the data tell us in terms of potential profit or loss based on the outcome of the analysis?

## 4.1  
A profit loss model will be built using the breakeven point of 24.13% and fixed cost, pricing, and solicitation costs.



4.2  We will look at the results of the previous analyses within the context of this model to give an estimate of the actual profit or loss given by variables noted in 4.1 and the product response confidence interval from SA2.



4.3  For respondents that indicate they would buy the USB Toaster we will determine the price range that was suitable for this population.  While the price is proprietary, this knowledge may inform the client as to whether or not the current estimated fixed price is appropriate or whether it might be increased.



4.4  Finally, we will look at variations in the model based on increasing the price of the USB Toaster in terms of the specified variables above.  Note, since these values are proprietary, the results of these analyses will be in terms of the variables and dollars.  The client may then use these calculations to determine the actual dollar profit or loss by entering different values of the variables (where appropriate into the equation).

```{r}

```


##Notes:  1) segment age and balance to make into ordinal factors
          2) re-run logistic regression
          3) make contact a binary (known/unknown) and perform logistic     
          regression to see what causes/correlates unknown contact.
          4) maybe look at the prediction for an individual with no debt, no house, single,
          young, tertiary education, etc - i.e., perfect buyer - and see prob they will buy.  ALso, look at 7 yeses from glm model.
          5) finally, maybe look at a model with just students and retirees?  How many retirees and students are in the gen pop?  Are these both demos worth chasing?
