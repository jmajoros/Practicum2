---
title: "P2 - TechR"
author: "Skip Potts"
date: "3/8/2021"
output: html_document
---
# Looking more at missing data from "contact"
```{r}
library(VIM)
library(mice)
setwd("C:/Users/skip/OneDrive/Desktop/Spring 21/S690/")
P2<-read.csv("C:/Users/skip/OneDrive/Desktop/Spring 21/S690/P2.csv")
P2$balance<-ifelse(P2$loan=="No",0,P2$balance)

#change unknown and blanks to NA's
dat2 <- read.csv("C:/Users/skip/OneDrive/Desktop/Spring 21/S690/P2.csv", header=T, na.strings=c(c("unknown", ""),"NA"))
#dropping non-obs responses
drop<-c("day", "month", "poutcome", "previous", "duration", "campaign", "pdays", "age", "balance", "price") #tracking our work, not necessarily relevant to our purposes.  I also removed variables which were complete.
dat2<-dat2[,!(names(dat2) %in% drop)]





Practicum_2_aggr = aggr(dat2, col=mdc(1:2), numbers=TRUE, sortVars=TRUE, labels=names(dat2), cex.axis=.7, gap=1, ylab=c("Proportion of missingness","Missingness Pattern")) #check proportion of missingness and missing patterns, contact has the most missing values

md.pattern(dat2, rotate.names = TRUE) #check the pattern of missing values, visual useless, console is useful

library(dplyr)
dat3<-filter(dat2, is.na(dat2$contact))
nrow(dat3)
drop<-c("contact") #removing contact
dat4<-dat3[,!(names(dat3) %in% drop)]
dat4<-na.omit(dat4)
dat4$y<-factor(dat4$y)
summary(dat4)
#still can't figure out why y is so strange on contact...
```

Contact behaves strangely missing nearly 4 times as much data as the next variable, but being the only one for which the level of unknown was truly significantly different.  Based on knowing that unknown contact is very different from other values, we might infer that people that say "no" are more likely to also not tell us their mode of contact - probably not the reverse.  We might also guess that contact is among the last pieces of info asked and people may have tired of answering at this point.  Essentially, people that are more willing to answer all or most of the questions, are more likely to say yes...but this doesn't really seem to add up either.  We may just need to say it is not understood, but there is a connection and it could be explored further in the future if the client is interested.

# Univriate EDA and Bivariate EDA 
```{r}
library(car)
hist(P2$age) # not great
hist(P2$balance) # not so good, seems like we have negatives coded on accident here.
P2$balance2<-abs(P2$balance)+1  #changing to absolute value
#maybe transform there using box-cox
bc = powerTransform(cbind(age, balance2) ~ 1, P2) #GS would not work here, maybe change to a factor...
summary(bc)
summary(P2) 
plot(P2$age,P2$balance )
plot(y~ education, P2)
plot(y~ job, P2)
plot(y~housing, P2)
plot(y~marital, P2)
plot(y~default, P2)  # we might infer here that unknown is usually "no" since the proportions match
plot(y~loan, P2)
plot(y~contact, P2) # so weird

```


#Specific Aim 1:  How are the two datasets (from Table 1 and Table 2)  different and why?

## 1.1 
Product Response will be considered as the response variable where “yes” means an individual indicated that they would purchase the USB Toaster and “no” indicated they would not.  Common variables between the two datasets are: age, marital status, profession, education level, whether an individual has a mortgage or not, whether an individual has a personal loan other than a mortgage, the balance of any non-mortgage personal loan, and primary phone (cell or telephone).  Additionally, whether an individual is 60 days delinquent or in credit default may also be comparable between datasets. 

Deleting binary variable on whether someone has a personal loan for two reasons:
1) we can get this info based on the balance
2) balance and loan don't agree.  Balance is probably the more useful (maybe true).
I figured this out MUCH later, so we might not have it here.  It makes sense to put in above area where we examine univariate/bivariate data.

## 1.2  
Common variables between the two datasets will be compared to determine whether they represent the same population.  We will use proportion tests that compare the percentages of certain types of demographic data and the total number of respondents for the dataset to determine if the datasets represent the same population or populations that are fundamentally different.
```{r}
#changing all to factor and merging blanks and unknowns into a single factor.
P2$job<-factor(sub("^$", "unknown", P2$job))
P2$marital<-factor(sub("^$", "unknown", P2$marital))
P2$education<-factor(sub("^$", "unknown", P2$education))
P2$default<-factor(sub("^$", "unknown", P2$default))
P2$housing<-factor(sub("^$", "unknown", P2$housing))
P2$loan<-factor(sub("^$", "unknown", P2$loan))
P2$contact<-factor(sub("^$", "unknown", P2$contact))
P2$y<-factor(P2$y)
summary(P2)

#prop.test to see if marital status is sig diff. in two pops.
prop.test(x=c(421*0.14,26543), n=c(421, 45211),
          conf.level=0.95)
#Very different

#prop.test to see if blue-collar is sig diff. in two pops.
prop.test(x=c(421*0.02,9064), n=c(421, 45211),
          conf.level=0.95)
#Very different

#prop.test to see if student is sig diff. in two pops.
table(P2$job)
prop.test(x=c(421*0.65,876), n=c(421, 45211),
          conf.level=0.95)
#Very different

#prop.test to see if unemployed/unknown is sig diff. in two pops.
table(P2$job)
prop.test(x=c(421*0.047,3309+1222), n=c(421, 45211),
          conf.level=0.95)
#Very different

#prop.test to see if education is sig diff. in two pops.
table(P2$education)
prop.test(x=c(421*0.83,13015), n=c(421, 45211),
          conf.level=0.95)
#Very different

#prop.test to see if mortgage is sig diff. in two pops.
prop.test(x=c(421*0.056,24656), n=c(421, 45211),
          conf.level=0.95)
#Very different

#prop.test to see if 60+/default is sig diff. in two pops.
prop.test(x=c(421*0.44,784), n=c(421, 45211),
          conf.level=0.95)
#Very different

#prop.test to see if contact is sig diff. in two pops.
prop.test(x=c(421*0.36,29262), n=c(421, 45211),
          conf.level=0.95)
#Very different
```
Here we can talk about how these are different and why.  Their population somehow is in debt, college, men, who use cells phones less, and are fiscally irresponsible...even if they want to buy this product, they may not have the money to do so.  

This gets to measure accuracy which we will talk about below in the psychometrics and other research (below)

## 1.3  
Based on the results of the above analyses, we will determine how and some potential reasons why the dataset are different (if we find they are) using both statistical reasoning and potentially some psychometric theory and research.

We can talk about my above notes here.  Sampling error skewing the population.  Also sources of error on psychological tests (intent to buy) time differences, personal variation, task not reflective of event.  More on this last one, research indicates surveys and focus groups aren't great barometers for purchasing.  A potential better metric will be discussed later, but we will move forward with the data we have.


### (Rough Draft of some text below. This might be better as part of the conclusion and suggesting a different approach to future research in this area.)

It has been noted that surveys asking if people will hypothetically buy a product with money they may not have, is not necessarily a good measure for intent to buy.  In researching the viability of this measure, our consulting team found a particular source of interest that specifically investigated durable homewares as part of its data.  According to research published in the Journal of the American Statistical Association by F. Thomas Juster, "consumer intentions to buy are inefficient predictors of purchase rates because they do not provide accurate estimates of mean purchase probability." (hyperlink: https://www.tandfonline.com/doi/abs/10.1080/01621459.1966.10480897) 

The reasons for this may be rooted in the fact that measuring an intent to purchase is a psychological construct and these measures have commonly known sources of unreliability*, all of which apply in this context.  The three common sources of performance variation are:
1) The person tested may change from one testing to the next
2) The task measured and the behavior may be different
3) The limited sample of the behavior results in an unstable and undependable score.

While it is not the intent to delve deeply into psychometrics, a brief aside exploring the potential reliability of the product intent variable in the context of the research above might be fruitful for understanding the task at hand of examining the probability of product success.

  In the article by Juster, the predominant source of error cited that made customer buying intentions problematic.  As Juster puts it, "intentions surveys cannot detect movements in mean probability among nonin- tenders, who account for the bulk of actual purchases and for most of the time-series variance in purchase rates." Meaning that though our data gives a "yes" or "no" response, the reality of the situation is different.  This gets at the heart of each of the above sources of unreliability.
  First, there is personal variation over time, whether through change in the person or the context of the persons life.  Second, there is a difference in task - what we really want is the probability of a purchase, but the data forces a binary situation on the individuals.  Finally, for each individual we have only one measure, which is not the most reliable measure.
  Juster's article finds that having individuals rate the probability they will purchase a product is a better measure of predictability.    While this does not completely fix any of the problems noted above, it allows for a better representation of the uncertainty in an individuals choice and thus makes for a better measure.

Footnote
$_{\textrm{*Thorndike and Thorndike-Christ, (2010).  Measurement and Evaluation in Psychology and Education (8th Ed.). Pearson}}$


## 1.4  
After viewing the results of the analyses above, we will determine how to proceed with each dataset in the subsequent specific aims.

We will proceed using our data set as the primary means of analysis, while also incorporating in the client data where suitable.

#Specific Aim 2:  What is the likelihood, based on all available data, that the USB Toaster will be a success?

## 2.1  
Using the data decided upon in specific aim 1 (SA1), we will focus on the average number of respondents that indicated they would purchase the USB Toaster.

```{r}
table(P2$y)
5289/39922 #positive product response
```

## 2.2  
Confidence intervals will be calculated for this average to give the potential range of sales within a 95% probability band.  We will generate confidence intervals based on both frequency of response in the final dataset and potentially a Bayesian estimate of the average using the data provided by the client as a baseline and adding to that the new information gathered in Table 2.

```{r}
#frequentist Table 1 and 2
data1<-c(replicate(212+5289, 1), replicate(209+39922, 0))
x.std.error <- sd(data1)/sqrt(45211+421)
mean(data1) + qt(0.025, 45211+420) * x.std.error
mean(data1) + qt(0.975, 45211+420) * x.std.error

#frequentist only Table 2
P2$y.n<-as.numeric(P2$y)-1
x.std.error <- sd(P2$y.n)/sqrt(45211)
lm(y.n~1, P2)
mean(P2$y.n) + qt(0.025, 45210) * x.std.error
mean(P2$y.n) + qt(0.975, 45210) * x.std.error

#BAYESIAN


#calcLikelihoodForProportion <- function(successes, total)
 # {
    # curve(dbinom(successes,total,x)) # plot the likelihood
#}
#calcLikelihoodForProportion(5289, 45211)

#credible interval
qbeta(c(0.025, 0.975),212+5289, 209+39922)

#plotting function for prior, likelihood and posterior
calcPosteriorForProportion <- function(successes, total, a, b)
  {
     # Adapted from triplot() in the LearnBayes package
     # Plot the prior, likelihood and posterior:
     likelihood_a = successes + 1; likelihood_b = total - successes + 1
     posterior_a = a + successes;  posterior_b = b + total - successes
     theta = seq(0.005, 0.995, length = 500)
     prior = dbeta(theta, a, b)
     likelihood = dbeta(theta, likelihood_a, likelihood_b)
     posterior  = dbeta(theta, posterior_a, posterior_b)
     m = max(c(prior, likelihood, posterior))
     plot(theta, posterior, type = "l", ylab = "Density",, xlab="Estimated Positive Product Response",
          lty = 2, lwd = 3, main = "", ylim = c(0, m), col = "red")
     lines(theta, likelihood, lty = 1, lwd = 3, col = "blue")
     lines(theta, prior, lty = 3, lwd = 3, col = "green")
     legend(x=0.75,y=m, c("Prior", "Likelihood", "Posterior"), lty = c(3, 1, 2),
          lwd = c(3, 3, 3), col = c("green", "blue", "red"))
     # Print out summary statistics for the prior, likelihood and posterior:
     calcBetaMode <- function(aa, bb) { BetaMode <- (aa - 1)/(aa + bb - 2); return(BetaMode); }
     calcBetaMean <- function(aa, bb) { BetaMean <- (aa)/(aa + bb); return(BetaMean); }
     calcBetaSd   <- function(aa, bb) { BetaSd <- sqrt((aa * bb)/(((aa + bb)^2) * (aa + bb + 1))); return(BetaSd); }
     prior_mode      <- calcBetaMode(a, b)
     likelihood_mode <- calcBetaMode(likelihood_a, likelihood_b)
     posterior_mode  <- calcBetaMode(posterior_a, posterior_b)
     prior_mean      <- calcBetaMean(a, b)
     likelihood_mean <- calcBetaMean(likelihood_a, likelihood_b)
     posterior_mean  <- calcBetaMean(posterior_a, posterior_b)
     prior_sd        <- calcBetaSd(a, b)
     likelihood_sd   <- calcBetaSd(likelihood_a, likelihood_b)
     posterior_sd    <- calcBetaSd(posterior_a, posterior_b)
     print(paste("mode for prior=",prior_mode,", for likelihood=",likelihood_mode,", for posterior=",posterior_mode))
     print(paste("mean for prior=",prior_mean,", for likelihood=",likelihood_mean,", for posterior=",posterior_mean))
     print(paste("sd for prior=",prior_sd,", for likelihood=",likelihood_sd,", for posterior=",posterior_sd))
}

#plotting the prior, likelihood and posterior
calcPosteriorForProportion(5289, 45211, 212, 209)

#credible interval
qbeta(c(0.025, 0.975),212+5289, 209+39922) # 95% of the time, we would expect the true mean would fall in this interval
qbeta(c(0.005, 0.995),212+5289, 209+39922) # 99% of the time, we would expect the true mean would fall in this interval
qbeta(c(0.0005, 0.9995),212+5289, 209+39922) # 999 out of 1000 times, we would expect the true mean would fall in this interval
```
Discuss the potential means here and meaning of Bayesian estimation of the mean.
	
## 2.3  
Using the calculated confidence intervals, we will determine the probability that the USB Toaster will be profitable.

```{r}
1-pbeta(.2413, 212+5289, 209+39922)
```
Essentially a 0% chance with the target market.  Hope is not lost, better targeting and price raising might lead to a profit.

# Specific Aim 3: What are some useful demographic data points that might help inform a potential marketing campaign?

## 3.1  
Regardless of the findings of SA2 (probability of product profit or loss), we will construct a logistic regression model using product response as the outcome variable and a training set of data (part of the dataset).  A logistic regression model will describe the change in odds of a success (indication of intent to purchase) based on changes in the other variables.   

```{r}
#create training set
library(car)
library(faraway)
# 50% of the sample size
sample_size <- floor(0.5 * nrow(P2))
# set the seed to make the partition reproducible
set.seed(538)
train_ind <- sample(seq_len(nrow(P2)), size = sample_size)
train <- P2[train_ind, ] #training
test <- P2[-train_ind, ] #test

summary(train)
m0<-glm(y.n~1, family=binomial, train)
m1<-glm(y.n~.-y-price-loan-balance, family=binomial, train) #removed price because it is only off interest later.  Removed loan for reasons stated above, and balance because balance2 is thought to be better measure (no negatives)
#(also the model didn't converge with price)
sumary(m1)
# LRT for null vs full - null hypothesis is null is good 
pchisq(deviance(m0)-deviance(m1),df.residual(m0)-df.residual(m1),lower=FALSE) #full is better
# G^2 goodness of fit - null is model fits
pchisq(deviance(m1),df.residual(m1),lower=FALSE) #no evidence to reject null that it is good fit
drop1(m1, test="Chi") #full model is best by AIC and deviance


m2<-glm(y.n~I(age^.17)+I(balance2^(-.15))+job+marital+education+default+housing+contact, family=binomial, train) #removed price because it is only off interest later.
sumary(m2)  #AIC indicates slightly better fit with tranformations.
drop1(m2, test="Chi") #age is not a significant predictors in this model.

m3<-glm(y.n~I(balance2^(-.15))+job+marital+education+housing+contact+default, family=binomial, train) 
summary(train$job)
sumary(m3) #AIC is slightly improved
drop1(m3, test="Chi") # m3 seems to be the best model
pchisq(deviance(m0)-deviance(m3),df.residual(m0)-df.residual(m3),lower=FALSE) #full is better
# G^2 goodness of fit - null is model fits
pchisq(deviance(m3),df.residual(m3),lower=FALSE) #no evidence to reject null that it is good fit
```
Model m3 is best (LRT based on deviance) and it fits very well according to G^2 test.
Notes: as balance increases, purchasing becomes more likely. Most likely job to buy is student, followed by retired, then unemployed (much lower) and admin finally.
Single most likely, with divorced next.  Tertiary education is best demographic, followed by unknown, and secondary.  no mortgage is best, contact cell is best - unknown is very bad, default unknown and no are similar (best).  Overall, we can see that strongest influences are detailed by coefficient above (note balance is different since it is a negative exponential).


3.2  Diagnostic analysis will be performed to check the assumptions of the logistic model using residual plots, halfnorm plots and other relevant tests. Potential outliers or influential points will be examined with caution.


# we might want to run these with a subsample to make them more visible.  Too many observations to see well here.  Maybe try with 500 randomly drawn?
```{r}
library(alr4)
library(faraway)
halfnorm(residuals(m3)) #looks okay for a binomial, no big outliers
plot(residuals(m3)) #these look pretty reasonable as far as we can tell
#residualPlots(m3)
influencePlot(m3)  #console info is better here perhaps.  We can see that nothing is too crazy here.
influenceIndexPlot(m3) # can't read these, too many.
```
For the most part, I think these look okay.


3.3 Tests will be conducted to see the accuracy of the model (correct classification rate) with respect to correctly identifying whether an individual will or will not purchase the product based on the other variables in the model using the test set (the data not used in the training set).

```{r}
train<-mutate(train, p.hat=predict(m3, type="response"), eta=predict(m3))
train <- mutate(train, predout=ifelse(p.hat < 0.5, "no", "yes"))
xtabs( ~ y + predout, train)
#correct spec rate for training 88.2%, but basically by saying no every time (1-0.117=0.883...)
(19940)/nrow(train)

thresh <- seq(0.01, 0.3, by = 0.01)
Class.rate <- numeric(length(thresh))
Sensitivity <- numeric(length(thresh))
Specificity <- numeric(length(thresh))
for(j in seq(along=thresh)){
  pp <- ifelse(train$p.hat < thresh[j],"no","yes")
  xx <- xtabs( ~ y + pp, train)
  Class.rate[j] <- (xx[1,1] + xx[2,2])/sum(xx)
  Sensitivity[j] <- xx[2,2]/(xx[2,1]+xx[2,2])
  Specificity[j] <- xx[1,1]/(xx[1,1]+xx[1,2])
}
matplot(thresh,cbind(Sensitivity,Specificity),type="l",xlab="Threshold",ylab="Proportion",lty=1:2)

plot(1-Specificity,Sensitivity,type="l")
abline(0,1,lty=2)
```
Not great at predicting...Specificity is good (red - % of time correctly classifying "no"), sensitivity is terrible (black - % of time correctly classifying "yes"). So we know some things that make people more likely to buy, but don't have a great model.  Perhaps contingency tables will be better....we could still see how it performs on the test set or just use the full set above.  (Good verbiage borrowed/altered from a text: Almost no cases are classified as "yes" because the probability of "no" always dominates the outcome. so the performance is not what we would like, but not surprising given the large proportion of "no" in each category.)


## 3.4  
Contingency tables will also be constructed to determine if particular combinations of variables are especially lucrative possibilities or potentially poor demographics for sales.  While binomial regression (two end values, yes and no) will give us odds changes based on variable changes, contingency tables can give us strong insight into the relationship between predictive variables that could help in mirco-targeting potential buyers (more below).

```{r}
ct.stuff<-function(fact1, fact2, data){
(ct = xtabs(P2$y.n ~ fact1 + fact2, data)) #yes count on education by employment
(ct2 = xtabs( ~ fact1 + fact2, data))
ct/ct2
}
summary(P2)
table(P2$education)
ct.stuff(P2$education, P2$job, P2) #shows percentage yes for cross section.  We see students say yes much more frequently overall.  Over 24.13%...only place we see this - basically college or grad students
#retired also approaches critical, and is over for tertiary
#unemployed tertiary also decent at 19%

ct.stuff(P2$education, P2$marital, P2)  #single-tertiary at 18.3%, 

ct.stuff(P2$education, P2$default, P2)  #tertiary better, but not great.

ct.stuff(P2$education, P2$housing, P2)  #tertiary and no house @ 20%

ct.stuff(P2$education, P2$contact, P2)  #tertiary again performing well.

ct.stuff(P2$job, P2$marital, P2) #retired but not single, students not divorced- especially single, single and unemployed

ct.stuff(P2$job, P2$default, P2) #students unless in default, similar to retired (not as good, but okay)

ct.stuff(P2$job, P2$housing, P2) #retired non-home-owner, student non-homeowner, and unemployed non-homeowner

ct.stuff(P2$job, P2$contact, P2)  #here's the caveat,  This weird group of unknown contacts
#other than this, students and retirees still come out okay.

ct.stuff(P2$marital, P2$default, P2)  #single is better...but nothing special

ct.stuff(P2$marital, P2$housing, P2)  #single non-homeowner, generally nonhomeowners are better

ct.stuff(P2$marital, P2$contact, P2) #single cell and divorced tele

ct.stuff(P2$default, P2$housing, P2) # no housing, no default.  We are starting to get a clearer picture of a demo type here.
# generally single, well-educated, students or retirees with no encumberance (financial or relationship).  Basically the picture of table 1.

ct.stuff(P2$default, P2$contact, P2) #cellular no default.  Again, unencumbered.

ct.stuff(P2$housing, P2$contact, P2) #non-homeowners - this product isn't about people on the go, it's about single people living in apartments.

str(P2)
```


3.5  The contingency tables generated will be analyzed further to determine if specific variables are independent of others or have a relationship.  These results may allow the consultants to modify certain variables to be more descriptive and explanatory within the dataset.  This will potentially illuminate some variable relationships further (such as marital status and education’s combined effect on intent to purchase).

Done above.

# 3.6  
The final models of both methods described above will be interpreted to determine any useful demographic data that may be used to maximize profit by targeted marketing.  In the case of a probable loss in section SA2, this may allow for stronger profit potential based on focusing on the demographics identified in the models that are positively associated with intent to purchase.

```{r}
sumary(m3)  #we see student/retired, tertiary, divorced/single, no house
#we can interpret these, but I don't think it is necessary
#these are all in line with the cts
# we can use the general direction and size from logistic regression, but with specific % from cts to discuss
#Unfortunately for the client, student + retired is less than 3k of a 45K+ sample and this may be the limiting factor. (roughly 13K have tertiary, over 17k single/divorced, over 24.5k no housing)
```
Let's look at a model composed only of students and retirees...
```{r}
Prime<-subset(P2, job=="student"|job=="retired", select = c(balance2,job,marital,education,housing,contact,default, y, price, y.n))
summary(Prime)
732/2254 #yes rate is good
ct= xtabs( ~ y + job, Prime)
ct
476/2110 #retiree purchase rate - still not good enough
256/(256+620)#student purchase rate - much better

#So...student only model
Prime<-subset(P2, job=="student", select = c(balance2,marital,education,housing,contact,default, y, price, y.n))
summary(Prime)
256/(256+620) #student purchase rate 41.3%  not TOO far from sample 1...could be explained by the skew towards men in sample 1, meaning that their numbers were possibly accurate for the pop they surveyed.

#let's rerun the bayseian stuff now...
#plotting the prior, likelihood and posterior
calcPosteriorForProportion(256, 256+620, 212, 209)
#posterior mean at 36% if we use their sample - probably shouldn't because it looks like gender has an influence.

#credible interval
qbeta(c(0.025, 0.975),212+256, 209+876) # 95% of the time, we would expect the true mean would fall in this interval

Prime<-subset(Prime, y.n==1, select = c( price))
summary(Prime)
P2.y<-subset(P2, y.n==1, select = c( price))
{
plot(density(Prime$price), col="red")
lines(density(P2.y$price), col="blue", add=T)
}

mean(P2.y$price) #mean estimated price for total pop of yeses
mean(Prime$price) # mean estimated price for student pop of yeses
```
So, it looks like we have a specific target market that this product *could* work for.  But the question is really if targeting colleges can achieve a high enough population to match the 5 major metros originally intended and will it change the costs (and effectively the breakeven point)?  This is something the client must answer, but IF they still want to move forward with the USB Toaster, we've given them a clear path where they could even have college logos on the outside of the toasters.

We can also see that students who said they would buy the product expect to pay about $28 more than the population of yeses on the whole (yeses there were higher than no's as well).  So it could justify raising the sale price which *may* make achieving a profit more likely.


# Specific Aim 4:  What does the data tell us in terms of potential profit or loss based on the outcome of the analysis?

## 4.1  
A profit loss model will be built using the breakeven point of 24.13% and fixed cost, pricing, and solicitation costs.

From the client info, we know that 24.13% is the break-even point of purchases.  We also know that price pre item, cost per item, and solicitation costs will be fixed.  From this we can construct the profit for a population of size $n$ (in this case the 5 major metropolitan areas) to be

$$Profit_n=nz(\% purchase-24.13)$$
where $z$ is the margin on one item (fixed price - fixed cost). This essentially gives us the profit per product sold times the difference of percentage of purchasers and the break-even point. Notably, this is for a particular fixed price for the USB toaster.  While solicitation costs and cost per item might be truly considered fixed, it is possible that the client, based on estimated pricing data gained from the sample, might choose to raise the potential fixed price.  In this case, raising the price of the product might lower the break-even sales point since 
$$Breakeven*nz=solicitation\,\, costs $$  
shows us that the break-even point and $z$ are inversely related - meaning when margin per item goes up, the break-even point goes down.  Unfortunately, the same is true for $n$ decreasing and the break-even point increasing.  

The pricing estimation information we've included may give additional insight into whether a change in price is justified based on the current proprietary price considered by the client.  We will be more than happy to run calculations and confidence intervals for both sales and profit should the client request this (ALTERNATIVELY We could actually model change in price effect on profit since it is just a calculation - but I'm not doing it right now. - like doubling price means halving the breakeven point, but in a graph from).  For now, we will proceed with all fixed pricing and costs unchanged to see if the product is likely to be profitable as is.

Recent estimates put the college population (hyperlink: https://www.census.gov/newsroom/press-releases/2019/school-enrollment.html) of the US at 18.9 million, while the five major metropolitan markets (NYC, LA, Chicago, Houston, and Pheonix - https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population) are estimated to have a combined population of 19 million.  Thus, we have good news for our client since these populations are roughly the same size and we have a larger than needed positive response rate to make the USB Toaster a profitable product.  Additionally, we can potentially regard the student population as larger given the metropolitan markets contain children that will not be part of the target market, whereas all college students are of an age to buy the USB Toaster.  This result does assume that this doesn't change the cost per item or the solicitation costs however, so the client is encouraged to use the models above to see if the USB Toaster remains profitable under a change in these assumptions.  The client is also more than welcome to contact our team to run additional models or calculations.  



```{r}
8336817+3979576+2693976+2320268+1680993 #five metro pops combined
```





4.2  We will look at the results of the previous analyses within the context of this model to give an estimate of the actual profit or loss given by variables noted in 4.1 and the product response confidence interval from SA2.

Using the credible interval of product sales from the student only population we will provide the calculation the 95% credible interval of profit or loss.

Lower bound of profit:
```{r}
18900000*(0.2787852-0.2413)
```

$708,470*z

Upper bound of profit:
```{r}
18900000*(0.3244038-0.2413)
```
$1,570,662*z

Maybe this is enough....?  I think I've addressed some of what is below already and we cannot know if an increase or decrease in price would change an individuals intent to purchase, so this should be a consideration on the part of the client when considering their current price and a change.  We suggest consulting us further with the proprietary price so that we might analyze potential effects of a different price after looking through the data with a particular focus on the density of the estimated sales prices.




4.3  For respondents that indicate they would buy the USB Toaster we will determine the price range that was suitable for this population.  While the price is proprietary, this knowledge may inform the client as to whether or not the current estimated fixed price is appropriate or whether it might be increased.

 ...

4.4  Finally, we will look at variations in the model based on increasing the price of the USB Toaster in terms of the specified variables above.  Note, since these values are proprietary, the results of these analyses will be in terms of the variables and dollars.  The client may then use these calculations to determine the actual dollar profit or loss by entering different values of the variables (where appropriate into the equation).

```{r}

```


##Notes:  1) make contact a binary (known/unknown) and perform logistic 
          regression to see what causes/correlates unknown contact.
          
